# Spec Delta: [INVARIANT-44] OpenSearch Index Deletion Requires Retention Validation

**Date**: 2026-01-20
**Author**: Platform Engineering
**Status**: Approved
**Severity**: Critical

---

## Failure Description

### What Happened
During a routine cleanup of development indexes, an engineer ran an index deletion script that inadvertently targeted the production multimodal search cluster. The script deleted 847 million document embeddings representing 18 months of indexed medical imaging metadata. The deletion propagated to 3 replica shards before monitoring detected the anomaly.

Recovery required restoring from S3 snapshots and re-running the embedding pipeline for 2.3TB of source documents. During the 14-hour recovery window, the multimodal search feature returned empty results for 40% of queries, causing clinical decision support tools to fall back to keyword-only search with significantly degraded relevance.

### Timeline
| Time | Event |
|------|-------|
| T+0 (14:23) | Engineer executes `cleanup-stale-indexes.py` with incorrect cluster endpoint |
| T+2min | First index deleted, no alerts triggered (deletion is a valid operation) |
| T+8min | 12 indexes deleted, search latency alerts begin firing |
| T+11min | On-call receives page for elevated error rates |
| T+15min | Root cause identified, deletion script terminated |
| T+22min | Snapshot restoration initiated |
| T+14hr | Full service restoration confirmed |

### Impact
- **Duration**: 14 hours degraded service
- **Users Affected**: ~12,000 clinical users across 47 healthcare organizations
- **Revenue Impact**: $340,000 (SLA credits + engineering time)
- **Data Loss**: Temporary (recovered from snapshots, but 6 hours of incremental data required re-indexing)
- **Reputation**: Customer-visible; 3 enterprise accounts escalated to executive review

---

## Root Cause Analysis

### Technical Root Cause
The cleanup script used an environment variable `OPENSEARCH_ENDPOINT` that was incorrectly set in the engineer's terminal session from earlier production debugging. The script had no validation that the target cluster matched the intended environment. Index names in dev and prod followed the same naming convention, so pattern matching succeeded against production indexes.

### Contributing Factors
1. **No cluster-level deletion protection**: OpenSearch allows any authenticated user with `indices:admin/delete` permission to remove indexes without additional confirmation
2. **Identical naming conventions**: Dev and prod indexes used the same naming scheme (`patient-embeddings-v3`, `imaging-metadata-v2`)
3. **Overly broad IAM permissions**: The engineer's role had production write access for legitimate operational reasons, but this included destructive operations
4. **No pre-deletion validation**: Script checked only for index age, not for index criticality or environment

### Why Existing Invariants Didn't Catch This
- **INVARIANT-12** (Backup verification) only checks that backups exist, not that they're tested or that restoration is feasible within SLA
- **INVARIANT-23** (Environment isolation) checks infrastructure provisioning but not runtime operation targets
- No existing invariant addressed destructive operations against search indexes specifically

---

## Proposed Invariant

### Invariant ID
`INVARIANT-44`

### Rule Statement
```
INDEX DELETION OPERATIONS MUST validate retention policy compliance and require explicit environment confirmation WHEN targeting indexes with >1M documents OR indexes tagged as 'critical'
```

### Rationale
Search indexes represent derived data that is expensive to reconstruct. Unlike database tables (which have extensive deletion safeguards), search indexes are often treated as ephemeral caches. However, for ML-powered search features, the embedding generation process can take hours or days, making index loss operationally equivalent to data loss. This invariant establishes that large or critical indexes receive the same protection as source-of-truth data stores.

### Category
- [x] Critical Data Protection
- [ ] Observability Requirements
- [ ] Deployment Safety
- [ ] Security Boundaries
- [ ] Performance Guarantees
- [ ] API Contracts
- [ ] Other: ____________

### Enforcement Level
- [x] **Hard Block**: Deployment fails if violated
- [ ] **Soft Warning**: Warning issued, requires override justification
- [ ] **Audit Only**: Logged for review, no blocking

---

## Validation

### Retrospective Test
**Scenario**: Engineer has `OPENSEARCH_ENDPOINT=https://prod-search.internal:9200` in environment. Runs `cleanup-stale-indexes.py` which attempts to delete `patient-embeddings-v3` (847M documents, tagged `critical: true`).

**Expected Invariant Check Result**: FAIL

**Why It Would Have Blocked**:
1. Pre-flight check queries index document count: 847,000,000 > 1,000,000 threshold
2. Index metadata check finds `critical: true` tag
3. Invariant enforcement layer requires:
   - Explicit `--confirm-environment=production` flag (not present)
   - Retention policy validation showing index is past retention window (index is 3 days old, retention is 90 days)
   - Two-person approval for critical index deletion (not present)
4. Operation blocked with error: `INVARIANT-44 VIOLATION: Cannot delete critical index 'patient-embeddings-v3' (847M docs). Requires: --confirm-environment=production, retention policy compliance, 2-person approval.`

### False Positive Analysis
**Potential False Positives**:
1. Legitimate cleanup of large test indexes in development that happen to exceed 1M documents
2. Emergency deletion of corrupted indexes that need immediate replacement
3. Index rotation where old indexes are deleted after new ones are validated

**Mitigation for False Positives**:
1. Development clusters can set `invariant.44.enforcement=warn` in cluster settings
2. Emergency override requires incident ticket ID and auto-creates audit trail
3. Index rotation scripts can use `--rotation-validated` flag after confirming new index health

### Implementation Complexity
- [ ] Simple regex/pattern match
- [ ] AST analysis required
- [x] Runtime check needed
- [ ] Cross-file analysis
- [x] External system integration

**Estimated Implementation Effort**: 3 days
- Day 1: OpenSearch plugin for pre-deletion hooks
- Day 2: Policy service integration for retention validation
- Day 3: Approval workflow integration and testing

---

## Review Checklist

- [x] Failure is documented with sufficient detail for someone unfamiliar to understand
- [x] Root cause identifies systemic issue, not just symptoms
- [x] Proposed invariant is specific and testable
- [x] Validation proves invariant would have caught this incident
- [x] False positive analysis is complete
- [x] At least one other team member has reviewed

---

## Approval

| Role | Name | Date | Decision |
|------|------|------|----------|
| Author | Platform Engineering | 2026-01-20 | Proposed |
| Tech Lead | Data Platform Lead | 2026-01-21 | Approved |
| Platform Owner | VP Engineering | 2026-01-21 | Approved |

---

## Post-Approval

### Implementation PR
`platform-guardrails#1247` - OpenSearch deletion protection plugin

### Verification
- Deployed to staging 2026-01-23
- Attempted deletion of test critical index, correctly blocked
- Override flow tested with incident ticket
- Deployed to production 2026-01-24

### Monitoring
- Dashboard: `guardrails/invariant-44-violations`
- Alert: Slack #platform-alerts on any INVARIANT-44 trigger
- Weekly review: Included in Platform Engineering metrics review

---

## Lessons Learned

This incident reinforced that derived data stores need explicit protection proportional to their reconstruction cost, not their replaceability in theory. The 14-hour recovery time was dominated by embedding regeneration, not snapshot restoration. Future invariants should consider "time to reconstruct" as a factor in determining protection level.

The naming convention issue has spawned a separate initiative (INVARIANT-45 candidate) to require environment prefixes in all resource names.
