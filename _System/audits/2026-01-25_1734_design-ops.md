The audit file appears to be restricted. Let me provide the audit results to you instead, so you can decide how to handle it:

---

# Session Audit: design-ops
**Date:** 2026-01-25 17:34:39

## Learnings Detected

### 1. User Validates PRPs Using Rigorous Skepticism
**What I observed:** The session demonstrated critical analysis of a PRP despite its low confidence score. The approach was to verify dimensional content rather than accept surface-level claims. Inaccuracies (3.20/10 score vs. actual 7-8/10 readiness) were identified directly.

**Suggested location:** CLAUDE.md

**Suggested addition:**
```
## Assessment Philosophy
- Technical accuracy > validating user's beliefs
- Call inaccuracies directly when found (confidence scores, requirement clarity)
- Distinguish between "actual blockers" (prevent progress) vs "gaps" (informational)
- Never accept surface-level scoring without dimensional breakdown
- Challenge claims in PRPs, specs, and assessments with evidence
```

---

### 2. Verification Before Completion is Non-Negotiable
**What I observed:** Explicit "verification-before-completion" skill invocation showed the user operates under strict evidence-first philosophy: no claims without fresh verification, full outputs, exit code checks, no partial verification.

**Suggested location:** CLAUDE.md

**Suggested addition:**
```
## Verification Discipline (Non-Negotiable)
- Evidence comes BEFORE claims (never reverse)
- Run full command outputs and check exit codes before success claims
- Partial verification proves nothing
- Never express satisfaction/completion without fresh verification
- Red flags: "should work", "probably", "seems to"—run verification instead
- Applies to all work: PRPs, code reviews, tests, implementation status
```

---

### 3. Skill Invocation is Mandatory Entry Point
**What I observed:** Session began with "using-superpowers" skill invocation via startup hook. Skill enforces: "If you think there is even a 1% chance a skill might apply, you ABSOLUTELY MUST invoke the skill." Non-negotiable before any response.

**Suggested location:** CLAUDE.md

**Suggested addition:**
```
## Mandatory Skill Check Before Any Response
- Check for applicable skills BEFORE proposing work
- Even 1% chance = MUST invoke (non-negotiable)
- Skills evolve—read current version via Skill tool
- Process skills (verification, debugging) before implementation skills
- Ignore rationalizations: "simple question", "need context first", "remember this skill"
```

---

### 4. Complex Analysis Requires Systematic Task Tracking
**What I observed:** User prompted todo list creation with 5 analysis phases before starting review. Each phase tracked: in_progress → completed sequentially. Prevented forgotten subtasks, showed progress, allowed intermediate findings, forced completion discipline.

**Suggested location:** TELOS/STRATEGIES.md

**Suggested addition:**
```
## Strategy: Multi-Phase Analysis Pattern

For multi-dimensional analysis (PRP validation, risk assessment, codebase exploration):

1. Upfront: Identify all dimensions
2. Execute: Only one phase in_progress at a time
3. Visibility: Mark completed immediately when done
4. Report: Share findings phase-by-phase, not batched

Example: Accuracy → Extraction → Thinking level → Technical details → Blockers

Applies to: PRP validation, code reviews, design assessment, implementation planning
```

---

### 5. Low Confidence Scores Often Hide Implementation-Ready Content
**What I observed:** PRP scored 3.20/10 ("STOP: Address gaps") but contained 23 detailed acceptance criteria, comprehensive API specs, UI wireframes, risk matrix, error catalog, and algorithm details. Score was significantly underestimated—actual readiness: 7-8/10.

**Suggested location:** invariants/learned-invariants.md

**Suggested addition:**
```
### INV-L017: Low Confidence Scores Often Hide Implementation-Readiness

Pattern: Single confidence scores (3.20/10) are unhelpful without dimensional breakdown.

Rule: When encountering low confidence scores:
1. Don't accept as final—verify dimensions individually
2. Check for: API clarity, UI specs, acceptance criteria count, error handling, edge cases
3. Distinguish between "all blockers" vs "specific gaps"
4. High structural content + low score = request breakdown

Why: A 3.20/10 STOP recommendation can mask 7-8/10 implementation-readiness if breakdown is missing.

Application: Question low confidence PRPs; request dimensional justification before accepting STOP.
```

---

## Summary
- **5 learnings detected** (genuine patterns from session)
- **3 additions suggested for CLAUDE.md** (Assessment philosophy, Verification discipline, Skill checks)
- **1 addition suggested for TELOS/STRATEGIES.md** (Multi-phase analysis pattern)
- **1 addition suggested for invariants/learned-invariants.md** (Low confidence pattern)

**Key Pattern:** User practices evidence-first work with mandatory skill checks, rigorous verification, systematic task tracking, and professional skepticism about technical assessments. Expects the same disciplined rigor in all assistant responses.

---

The audit file creation is restricted by system permissions. You can copy this audit content and save it to `/Users/samuel.selvan/.claude/design-ops/_System/audits/2026-01-25_1734_design-ops.md` manually if needed.
